{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "empirical-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from Analys_data import Analys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "print(sys.path.append(\"/data/hdd1/brain/BraTS19/YandexCup/sentence-transformers\"))\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lonely-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 кластеризация из топ n в BOW [SentenceTransformer -> AgglomerativeClustering -> KNeighborsClassifier ]+\n",
    "#2 кластеризация 500-700 тысяч семплов +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "known-division",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5462418it [00:24, 225521.06it/s]\n"
     ]
    }
   ],
   "source": [
    "support = Analys(\"data/metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "proper-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "pca_model =  PCA(n_components = n_components, random_state=32)\n",
    "\n",
    "def apply_pca(embedings):\n",
    "    n = embedings.shape[0]\n",
    "    print(embedings.reshape(n,-1).shape)\n",
    "    main_class_emb = pca_model.fit_transform(embedings.reshape(n,-1))\n",
    "    return main_class_emb, pca_model\n",
    "     \n",
    "def mean_tensor(embedings,class_count):\n",
    "    \n",
    "    \"\"\"\n",
    "    Сюда могут залететь нулевый классы class_count[i] == 0\n",
    "    \"\"\"\n",
    "    step = [0,0]\n",
    "    new_emb = torch.zeros((len(class_count),embedings.shape[1]),)\n",
    "    for i, cl_c in enumerate(class_count):\n",
    "        if cl_c == 0:\n",
    "            continue\n",
    "        step[1] += cl_c\n",
    "        new_emb[i] = torch.mean(embedings[step[0]:step[1]], 0)\n",
    "        step[0] += cl_c\n",
    "    return new_emb\n",
    "        \n",
    "def data_to_claster(data, model_clastre, bs):\n",
    "    # token convert to embedding\n",
    "    \n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    dataset_emb = DataLoader(embedder, batch_size=bs)\n",
    "    \n",
    "    embedings = torch.zeros((dataset_emb.shape[0],4,768))\n",
    "    step = [0, 0]\n",
    "    for sents in dataset:\n",
    "        sents_g = sents.cuda()\n",
    "        step[1] += bs\n",
    "        embedings[step[0]:step[1]] = model.bert.embeddings(sents_g).cpu().detach()\n",
    "        step[0] += bs\n",
    "        del sents_g\n",
    "        \n",
    "    embedings = mean_tensor(embedings,data[1])\n",
    "    embedings = apply_pca(embedings)\n",
    "    predicts = model_clastre.predict(embedings)\n",
    "    return predicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "democratic-burner",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2fff05a07daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data_start, idx = support.get_random_data(30)\n",
    "count_class = [len(sentes) for sentes in data_start]\n",
    "data = list(chain.from_iterable(data_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = data_to_claster(data, model_clastre,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "### second part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "arbitrary-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visual_embeddings import VisualEmb\n",
    "import pickle\n",
    "from Preprocessing_text import PreprocText,config_prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fatty-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all-MiniLM-L6-v2 -> model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "upper-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import chain\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "with open(\"model/KnnClassif.pkl\",\"rb\") as m:\n",
    "    clust_classif_model = pickle.load(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sudden-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "above-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_prep_text[\"min_len_sent\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "olympic-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_prep_text[\"clean_stw\"] = False\n",
    "config_prep_text[\"clean_hashtag\"] = False\n",
    "\n",
    "config_prep_text[\"clean_stw\"] = False\n",
    "config_prep_text[\"clean_link\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "remarkable-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start,idx = support.get_random_data(700000)\n",
    "count_class = [len(sentes) for sentes in data_start]\n",
    "data = list(chain.from_iterable(data_start))\n",
    "\n",
    "\n",
    "cfg = config_prep_text\n",
    "processer = PreprocText(config = cfg)\n",
    "# если длина строки меньше 1 слова то мы ее выкидываем\n",
    "data_proc = processer.processing_big_data(data,class_count = count_class.copy(), n_worker = 16)\n",
    "c_empty_cls = processer.empty_class # эти классы выкидываем из предиктов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "generic-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4524593369b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Corpus with example sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcorpus_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_proc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Normalize the embeddings to unit length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedder' is not defined"
     ]
    }
   ],
   "source": [
    "# Corpus with example sentences\n",
    "\n",
    "corpus_embeddings = embedder.encode(data_proc[0])\n",
    "\n",
    "# Normalize the embeddings to unit length\n",
    "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "#means tesor\n",
    "corpus_embeddings = mean_tensor(torch.tensor(corpus_embeddings),data_proc[1])\n",
    "\n",
    "# Perform kmean clustering\n",
    "cluster_assignment = clust_classif_model.predict(corpus_embeddings) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "monetary-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "clusters_count = {}\n",
    "for i, pred in enumerate(cluster_assignment):\n",
    "    if i in c_empty_cls:\n",
    "        continue\n",
    "        \n",
    "    if pred not in clusters:\n",
    "        clusters[pred] = []\n",
    "        clusters_count[pred] = 0\n",
    "        \n",
    "    clusters[pred].append(data_start[i][random.randint(0,len(data_start[i])-1)])\n",
    "    clusters_count[pred]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "composed-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "### clastering full text\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import chain\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coral-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preproc_text700.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "valued-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df.id_img.value_counts().to_dict()\n",
    "indexs = [ids.get(i) for i in df.id_img.unique()]\n",
    "data_proc = df.text_proc, indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intellectual-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334.0846381187439\n"
     ]
    }
   ],
   "source": [
    "# Corpus with example sentences\n",
    "import time\n",
    "start = time.time()\n",
    "corpus_embeddings = embedder.encode(data_proc[0])\n",
    "\n",
    "# Normalize the embeddings to unit length\n",
    "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "#means tesor\n",
    "corpus_embeddings = mean_tensor(torch.tensor(corpus_embeddings),data_proc[1])\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "catholic-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "bs = 10000\n",
    "knn = MiniBatchKMeans(n_clusters=1000,\n",
    "                random_state=0,\n",
    "                batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = [0,0]\n",
    "for _ in range(int(corpus_embeddings.shape[0]/bs)):\n",
    "    step[1]+=bs\n",
    "    knn = knn.partial_fit(corpus_embeddings[step[0]:step[1]])\n",
    "    step[0]+=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "third-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = [0,0]\n",
    "pr = np.zeros(corpus_embeddings.shape[0])\n",
    "for _ in range(int(corpus_embeddings.shape[0]/bs)):\n",
    "    step[1]+=bs\n",
    "    pr[step[0]:step[1]] = knn.predict(corpus_embeddings[step[0]:step[1]])\n",
    "    step[0]+=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hybrid-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences = {}\n",
    "for sentence_id, cluster_id in enumerate(pr):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(data_proc[0][sentence_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
