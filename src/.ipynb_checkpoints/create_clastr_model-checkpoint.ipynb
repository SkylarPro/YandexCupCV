{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "allied-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "print(sys.path.append(\"/data/hdd1/brain/BraTS19/YandexCup/CLIP\"))\n",
    "import clip\n",
    "from Visual_embeddings import VisualEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amino-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = clip.load(\"RN50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "royal-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clasters_data/BOW2_3mln.csv\")\n",
    "data = df.Text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "annoying-letter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:01<00:00, 19835.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 4, 768])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token convert to embedding\n",
    "data_token = clip.tokenize(data)\n",
    "bs = 4\n",
    "dataset = DataLoader(data_token, batch_size=bs)\n",
    "\n",
    "embedings = torch.zeros((data_token.shape[0],4,768))\n",
    "step = [0, 0]\n",
    "for sents in dataset:\n",
    "    sents_g = sents.cuda()\n",
    "    step[1] += bs\n",
    "    embedings[step[0]:step[1]] = model.bert.embeddings(sents_g).cpu().detach()\n",
    "    step[0] += bs\n",
    "    del sents_g\n",
    "embedings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sharp-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = embedings.shape[0]\n",
    "n_components = 400\n",
    "pca_model = PCA(n_components = n_components, random_state=32)\n",
    "\n",
    "main_class_emb = pca_model.fit_transform(embedings.reshape(n,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "increasing-mention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=40, random_state=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=40, random_state=0)\n",
    "kmeans.fit(main_class_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "worst-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kmeansv2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(kmeans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comprehensive-equation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### second part\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "print(sys.path.append(\"/data/hdd1/brain/BraTS19/YandexCup/sentence-transformers\"))\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "promotional-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clastering BOW top N elements -> and learning KNeighborsClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vulnerable-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clasters_data/BOW2_3mln.csv\")\n",
    "data = df.Text.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "running-softball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed compute emb\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "\n",
    "corpus_embeddings = embedder.encode(data)\n",
    "\n",
    "# Normalize the embeddings to unit length\n",
    "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "print(f\"Completed compute emb\")\n",
    "\n",
    "Perform kmean clustering\n",
    "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.) #, affinity='cosine', linkage='average', distance_threshold=0.4)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(data[sentence_id])\n",
    "\n",
    "for i, cluster in clustered_sentences.items():\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "useful-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###save inform about clasters\n",
    "from itertools import chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "renewable-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(chain.from_iterable([[(sent,label) for sent in sents]for label, sents in clustered_sentences.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "answering-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict({\"Sent\":[i[0] for i in k],\n",
    "                       \"Calster\":[i[1] for i in k],\n",
    "                       }).to_csv(\"ClastersInform.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "prompt-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "###create model\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "defensive-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasters_to_dataclass(clustered_s):\n",
    "    X = np.zeros((20000,384)) #feature\n",
    "    y = np.zeros(20000, dtype=np.int32) # clasters\n",
    "    step = [0, 0]\n",
    "    for ind in set(clustered_s.keys()):\n",
    "        corpus_embeddings = embedder.encode(clustered_s[ind])\n",
    "        corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True) \n",
    "        step[1] += corpus_embeddings.shape[0]\n",
    "        X[step[0]:step[1]] = corpus_embeddings\n",
    "        y[step[0]:step[1]] = ind\n",
    "        step[0] += corpus_embeddings.shape[0]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "naughty-thread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['на рабочий стол', 'обои на рабочий', 'обои для рабочий', 'рабочий стол на', 'рабочий стол эстетика', 'обои рабочий стол', 'рабочий стол обои', 'рабочий стол скачать', 'рабочий стол из', 'рабочий стол стиль', 'рабочий стол пк', 'рабочий стол ноутбук']\n"
     ]
    }
   ],
   "source": [
    "X,y = clasters_to_dataclass(clustered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "recovered-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "reduced-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "miniature-youth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6276"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 6) # 6-7\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "pr = model.predict(X_test)\n",
    "accuracy_score(pr,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "public-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open(\"KnnClassif.pkl\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
