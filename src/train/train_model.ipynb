{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "handled-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Sent2textDataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from clip.evaluate.utils import (\n",
    "    load_weights_only,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conservative-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    val_loss_hist = []\n",
    "    metric_y_val = metric_p_val = None\n",
    "    \n",
    "#     scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,\n",
    "#                                                   T_0=40, \n",
    "#                                                   T_mult=2,\n",
    "#                                                   eta_min=1e-9)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        \n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        loss_accum = 0\n",
    "        for i_step, (imgs, text_vect, att),label ,_  in enumerate(train_loader):\n",
    "                \n",
    "                imgs_gpu = imgs.to(device=device)\n",
    "                text_vect_gpu = text_vect.to(device=device)\n",
    "                att_gpu = att.to(device=device)\n",
    "                \n",
    "                y_gpu = label.to(device=device)\n",
    "                \n",
    "                _, prediction = model(x_gpu) # logits_per_text\n",
    "                loss_value = loss(prediction, y_gpu.reshape(-1))\n",
    "                _, preds = torch.max(prediction, 1)\n",
    "                \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "                if i_step == 0 and epoch == 0:\n",
    "                    metric_y = y_gpu.reshape((-1,)).cpu().numpy()\n",
    "                    metric_p = preds.cpu().numpy()\n",
    "                else:\n",
    "                    metric_y = np.concatenate((metric_y, y_gpu.reshape((-1,)).cpu().numpy()))\n",
    "                    metric_p = np.concatenate((metric_p, preds.cpu().numpy())) \n",
    "                    \n",
    "                correct_samples += torch.sum(preds == y_gpu.reshape((-1,)))\n",
    "                loss_accum += loss_value\n",
    "                \n",
    "                total_samples += y_gpu.shape[0]\n",
    "                del x_gpu\n",
    "                del y_gpu\n",
    "                #gc.collect()\n",
    "        \n",
    "        \n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = correct_samples / total_samples\n",
    "#         writer.add_scalar(\"Loss/train\", ave_loss, epoch)\n",
    "#         writer.add_scalar(\"Acc/train\", train_accuracy, epoch)\n",
    "#         writer.add_scalar(\"F1/train\", f1_score(metric_y,metric_p), epoch)\n",
    "\n",
    "        val_accuracy, loss_val,metric_y_val, metric_p_val = compute_valid(model, val_loader, loss, epoch,metric_y_val, metric_p_val)\n",
    "#         writer.add_scalar(\"Loss/valid\", loss_val, epoch)\n",
    "#         writer.add_scalar(\"Acc/valid\", val_accuracy, epoch)\n",
    "#         writer.add_scalar(\"F1/valid\", f1_score(metric_y_val,metric_p_val), epoch)\n",
    "        \n",
    "#         writer.add_scalar(\"Lr/epoch\", scheduler.get_last_lr()[-1], epoch)\n",
    "        scheduler.step(epoch)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        val_loss_hist.append(loss_val)\n",
    "\n",
    "        print(\"Average loss: %f, Val loss: %f, Train accuracy: %f, Val accuracy: %f, Train F1: %f,Val F1: %f\" % (ave_loss,loss_val, train_accuracy, val_accuracy,f1_score(metric_y,metric_p), f1_score(metric_y_val,metric_p_val)))\n",
    "        \n",
    "#         print('Epoch:', epoch, 'LR:', scheduler.get_last_lr())\n",
    "\n",
    "def compute_valid(model, loader, loss,epoch, metric_y= None, metric_p=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        loss_accum = 0\n",
    "        \n",
    "        for i_step, (x, y) in enumerate(loader):\n",
    "            x_gpu = x.to(device=device, dtype=torch.float)\n",
    "            y_gpu = y.to(device=device,)\n",
    "\n",
    "            _, prediction = model(x_gpu)\n",
    "            loss_value = loss(prediction, y_gpu.reshape((-1,)))\n",
    "            _, preds = torch.max(prediction, 1)\n",
    "            \n",
    "            \n",
    "            if i_step == 0 and epoch == 0:\n",
    "                metric_y = y_gpu.reshape((-1,)).cpu().numpy()\n",
    "                metric_p = preds.cpu().numpy()\n",
    "            else:\n",
    "                metric_y = np.concatenate((metric_y, y_gpu.reshape((-1,)).cpu().numpy()))\n",
    "                metric_p = np.concatenate((metric_p, preds.cpu().numpy())) \n",
    "            \n",
    "            \n",
    "            correct_samples += torch.sum(preds == y_gpu.reshape((-1,)))\n",
    "            total_samples += y_gpu.shape[0]\n",
    "            loss_accum += loss_value\n",
    "\n",
    "            del x_gpu\n",
    "            del y_gpu\n",
    "                \n",
    "        loss_val = loss_accum / (i_step + 1)\n",
    "        val_accuracy = correct_samples / total_samples\n",
    "        return val_accuracy, loss_val, metric_y, metric_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brave-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomDataset import Sent2textDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "heavy-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor.kumpan/.miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "5462418it [00:11, 456012.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/images\n",
      "From 50971 sample folder hasn't 50038\n"
     ]
    }
   ],
   "source": [
    "path_t_csv = \"data/60k_data_preproc.csv\" #pd.read_csv(\"data/60k_data_preproc.csv\")[:1000]\n",
    "path_i_json = \"data/images.json\"\n",
    "path_i_folder = \"data/images\"\n",
    "\n",
    "\n",
    "# в csv file для каждого касса должен быть массив\n",
    "model, args = load_weights_only(\"ViT-B/32-small\",seq_length = 15)\n",
    "\n",
    "ds = Sent2textDataset(path_t_csv,\n",
    "                      path_i_json,path_i_folder,\n",
    "                      down_data = False,\n",
    "                      n_classes = 5,args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "compressed-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda().float().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "funny-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "qualified-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (imgs, text_vect, att),label,_ in dl:\n",
    "    pr = model(\n",
    "        img_input={\"x\": torch.squeeze(imgs,1)},\n",
    "        text_input={\"x\": text_vect, \"attention_mask\":att})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "effective-nomination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-legislature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
