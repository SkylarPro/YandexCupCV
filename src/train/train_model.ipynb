{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handled-difference",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor.kumpan/.miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import Sent2textDataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/data/hdd1/brain/BraTS19/YandexCup/ru-clip\")\n",
    "\n",
    "import pandas as pd\n",
    "from clip.evaluate.utils import (\n",
    "    load_weights_only,\n",
    "    show_similarity,\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "from CustomDataset import Sent2textDataset\n",
    "model, args = load_weights_only(\"ViT-B/32-small\",seq_length = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secondary-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "def sempler(data_train, batch_size = 4,split = .8):\n",
    "    \n",
    "    data_size = len(data_train)\n",
    "\n",
    "    validation_split = split\n",
    "    split = int(np.floor(validation_split * data_size))\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices,val_indices = indices[split:],indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                              sampler=train_sampler,)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                            sampler=val_sampler,)\n",
    "\n",
    "    return train_loader,val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "conservative-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    val_loss_hist = []\n",
    "    metric_y_val = metric_p_val = None\n",
    "    \n",
    "#     scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,\n",
    "#                                                   T_0=40, \n",
    "#                                                   T_mult=2,\n",
    "#                                                   eta_min=1e-9)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        \n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        loss_accum = 0\n",
    "        for i_step, data  in enumerate(train_loader):\n",
    "            \n",
    "            \n",
    "                imgs_gpu = torch.squeeze(data[0].cuda(),1)\n",
    "                texts_gpu = data[1].cuda()\n",
    "                att_mask_gpu = data[2].cuda()\n",
    "                label_gpu = torch.arange(data[0].shape[0]).cuda()\n",
    "                \n",
    "                \n",
    "                _, prediction = model(img_input={\"x\": imgs_gpu},\n",
    "                                    text_input={\"x\": texts_gpu, \"attention_mask\":att_mask_gpu})\n",
    "                \n",
    "            \n",
    "                loss_value = loss(prediction, label_gpu)\n",
    "                \n",
    "                _, preds = torch.max(prediction, 1)\n",
    "                \n",
    "                print(preds)\n",
    "                \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "                if i_step == 0 and epoch == 0:\n",
    "                    metric_y = label_gpu.cpu().numpy()\n",
    "                    metric_p = preds.cpu().numpy()\n",
    "                else:\n",
    "                    metric_y = np.concatenate((metric_y, label_gpu.cpu().numpy()))\n",
    "                    metric_p = np.concatenate((metric_p, preds.cpu().numpy())) \n",
    "                    \n",
    "                correct_samples += torch.sum(preds == label_gpu)\n",
    "                loss_accum += loss_value\n",
    "                total_samples += label_gpu.shape[0]\n",
    "                \n",
    "                del imgs_gpu\n",
    "                del texts_gpu\n",
    "                del att_mask_gpu\n",
    "            \n",
    "                del label_gpu\n",
    "        \n",
    "        \n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = correct_samples / total_samples\n",
    "#         writer.add_scalar(\"Loss/train\", ave_loss, epoch)\n",
    "#         writer.add_scalar(\"Acc/train\", train_accuracy, epoch)\n",
    "#         writer.add_scalar(\"F1/train\", f1_score(metric_y,metric_p), epoch)\n",
    "\n",
    "        val_accuracy, loss_val,metric_y_val, metric_p_val = compute_valid(model, val_loader, loss, epoch,metric_y_val, metric_p_val)\n",
    "#         writer.add_scalar(\"Loss/valid\", loss_val, epoch)\n",
    "#         writer.add_scalar(\"Acc/valid\", val_accuracy, epoch)\n",
    "#         writer.add_scalar(\"F1/valid\", f1_score(metric_y_val,metric_p_val), epoch)\n",
    "        \n",
    "#         writer.add_scalar(\"Lr/epoch\", scheduler.get_last_lr()[-1], epoch)\n",
    "#         scheduler.step(epoch)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        val_loss_hist.append(loss_val)\n",
    "        \n",
    "\n",
    "        print(\"Average loss: %f, Val loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss,loss_val, train_accuracy, val_accuracy))\n",
    "    return metric_y_val, metric_p_val\n",
    "#         print('Epoch:', epoch, 'LR:', scheduler.get_last_lr())\n",
    "\n",
    "def compute_valid(model, loader, loss,epoch, metric_y= None, metric_p=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        loss_accum = 0\n",
    "        \n",
    "        for i_step, data in enumerate(loader):\n",
    "            \n",
    "            imgs_gpu = torch.squeeze(data[0].cuda(),1)\n",
    "            texts_gpu = data[1].cuda()\n",
    "            att_mask_gpu = data[2].cuda()\n",
    "            label_gpu = torch.arange(data[0].shape[0]).cuda()\n",
    "\n",
    "            _, prediction = model(img_input={\"x\": imgs_gpu},\n",
    "                                    text_input={\"x\": texts_gpu, \"attention_mask\":att_mask_gpu})\n",
    "            \n",
    "            \n",
    "            loss_value = loss(prediction, label_gpu)\n",
    "            _, preds = torch.max(prediction, 1)\n",
    "            print(preds.cpu().numpy().tolist())\n",
    "            \n",
    "            if i_step == 0 and epoch == 0:\n",
    "                metric_y = label_gpu.cpu().numpy()\n",
    "                metric_p = preds.cpu().numpy()\n",
    "            else:\n",
    "                metric_y = np.concatenate((metric_y, label_gpu.cpu().numpy()))\n",
    "                metric_p = np.concatenate((metric_p, preds.cpu().numpy())) \n",
    "            \n",
    "            \n",
    "            correct_samples += torch.sum(preds == label_gpu)\n",
    "            total_samples += label_gpu.shape[0]\n",
    "            loss_accum += loss_value\n",
    "\n",
    "            del imgs_gpu\n",
    "            del texts_gpu\n",
    "            del att_mask_gpu\n",
    "            \n",
    "            del label_gpu\n",
    "                \n",
    "        loss_val = loss_accum / (i_step + 1)\n",
    "        val_accuracy = correct_samples / total_samples\n",
    "        return val_accuracy, loss_val, metric_y, metric_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "heavy-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5462418it [00:12, 445537.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broken 0 images\n",
      "2278 images in folder from 2500 in csv file\n"
     ]
    }
   ],
   "source": [
    "path_t_csv = pd.read_csv(\"data/60k_data_preproc.csv\")[:2500] #\n",
    "path_i_json = \"data/images.json\"\n",
    "path_i_folder = \"data/images\"\n",
    "\n",
    "\n",
    "# в csv file для каждого касса должен быть массив\n",
    "\n",
    "\n",
    "ds = Sent2textDataset(path_t_csv,\n",
    "                      path_i_json,path_i_folder,\n",
    "                      down_data = False,\n",
    "                      n_classes = 5,args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hidden-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "documented-arabic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logit_scale',\n",
       " 'visual_encoder.projection.linear1.weight',\n",
       " 'visual_encoder.projection.linear2.weight',\n",
       " 'visual_encoder.projection.layer_norm.weight',\n",
       " 'visual_encoder.projection.layer_norm.bias',\n",
       " 'text_encoder.model.h.11.ln_1.weight',\n",
       " 'text_encoder.model.h.11.ln_1.bias',\n",
       " 'text_encoder.model.h.11.attn.c_attn.weight',\n",
       " 'text_encoder.model.h.11.attn.c_attn.bias',\n",
       " 'text_encoder.model.h.11.attn.c_proj.weight',\n",
       " 'text_encoder.model.h.11.attn.c_proj.bias',\n",
       " 'text_encoder.model.h.11.ln_2.weight',\n",
       " 'text_encoder.model.h.11.ln_2.bias',\n",
       " 'text_encoder.model.h.11.mlp.c_fc.weight',\n",
       " 'text_encoder.model.h.11.mlp.c_fc.bias',\n",
       " 'text_encoder.model.h.11.mlp.c_proj.weight',\n",
       " 'text_encoder.model.h.11.mlp.c_proj.bias',\n",
       " 'text_encoder.projection.linear1.weight',\n",
       " 'text_encoder.projection.linear2.weight',\n",
       " 'text_encoder.projection.layer_norm.weight',\n",
       " 'text_encoder.projection.layer_norm.bias']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.cuda().float()\n",
    "for name,param in model.named_parameters():\n",
    "    if name.find(\"visual_encoder\") != -1:\n",
    "        param.requires_grad = False\n",
    "    if name.find(\"text_encoder\")!= -1:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    if name.find(\"11\")!=-1 and name.find(\"visual_encoder\")== -1:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    if name.find(\"projection\")!=-1:\n",
    "        param.requires_grad = True\n",
    "    if name == \"logit_scale\":\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    \n",
    "[name for name,param in model.named_parameters() if param.requires_grad ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "logical-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLoss()\n",
    "optimizer = Adam([param for param in model.parameters() if param.requires_grad], lr = 1e-04, weight_decay = 1e-05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "minute-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,val_dl =  sempler(ds, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "coated-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([ 0,  3,  2,  3,  4,  5, 15,  7,  8,  9, 17, 19, 12, 13, 13, 17, 16, 15,\n",
      "        19, 16], device='cuda:0')\n",
      "tensor([19, 16,  2,  3,  4, 19,  6,  7,  8,  9, 15,  5, 12, 13,  1, 15,  6, 17,\n",
      "         0, 14], device='cuda:0')\n",
      "tensor([ 0,  1,  2,  4,  4,  3,  6,  7,  6,  9,  1, 11, 12, 12,  4,  0, 16, 17,\n",
      "        12, 19], device='cuda:0')\n",
      "tensor([ 4,  1,  3,  3,  4,  3,  6,  9,  8,  7, 10, 11, 17, 13, 14, 15, 17, 17,\n",
      "        18,  1], device='cuda:0')\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  4,  8, 18, 10, 11,  8,  8,  0, 15, 16, 17,\n",
      "        18, 15], device='cuda:0')\n",
      "tensor([ 0, 11,  5,  3,  4,  5,  6, 17,  8, 15, 10, 11, 12, 13, 14, 15, 12, 17,\n",
      "        12, 19], device='cuda:0')\n",
      "tensor([ 0,  1,  2,  3,  4, 11,  6, 15,  8, 18, 10, 11, 12, 13,  3, 12, 15, 17,\n",
      "        18,  1], device='cuda:0')\n",
      "tensor([14,  1,  2,  3,  5,  5, 11,  7, 19,  9, 10, 11, 12,  6, 14, 15,  1, 12,\n",
      "        19, 19], device='cuda:0')\n",
      "tensor([13,  1, 13,  3,  0,  4,  6,  7,  8, 13,  6, 11, 12, 14, 14, 15, 16, 17,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 8,  1,  2,  8,  4,  5,  6,  7,  8,  9,  9, 15, 14, 13, 14, 15, 16, 17,\n",
      "         6, 19], device='cuda:0')\n",
      "tensor([ 0, 18,  3,  3,  4,  6,  6,  9,  8,  9,  2, 11, 12, 13, 14, 10, 16, 11,\n",
      "        18, 13], device='cuda:0')\n",
      "tensor([ 0, 15, 15,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  1,  9, 17,\n",
      "         3,  7], device='cuda:0')\n",
      "tensor([ 0,  1,  2,  3,  4,  2,  6,  6,  0,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 0,  1, 18,  3,  4,  5,  2, 19,  8,  9,  1, 11,  9, 12, 19, 18, 16, 17,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 0,  1,  2, 19,  4, 18,  6,  7, 15, 12, 10, 11,  9, 13, 14, 18, 15, 17,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 0, 17, 14,  3,  4, 14, 11,  8,  0, 19, 10, 11, 14, 14, 14, 13, 16, 17,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 0,  9,  2,  3, 19,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  5, 17,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 0,  1, 16,  3,  3,  9, 17, 10,  0,  9, 10, 11, 12, 14, 14, 10, 16,  6,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 6,  1,  6,  3,  4,  1,  6,  7,  8,  9, 19, 11,  5, 13, 14, 15, 16, 17,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 0,  4,  2,  3,  4,  5,  6,  7,  8,  9, 18, 11, 12,  8, 14, 13,  4,  4,\n",
      "         2, 19], device='cuda:0')\n",
      "tensor([ 0, 16,  2,  3,  4,  3,  5, 18,  8,  9, 10, 11,  1, 13, 14,  5, 19, 16,\n",
      "        18, 19], device='cuda:0')\n",
      "tensor([ 5,  8,  2, 18,  4, 16,  1,  5, 17,  5, 10, 11,  2, 13, 11,  1, 16, 17,\n",
      "        18,  8], device='cuda:0')\n",
      "tensor([ 0,  1,  8,  3,  1,  0,  6,  7,  8,  5,  3, 11,  0, 13,  3, 15],\n",
      "       device='cuda:0')\n",
      "[0, 1, 11, 3, 4, 1, 7, 7, 8, 5, 10, 11, 12, 13, 7, 13, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4, 8, 6, 1, 5, 1, 17, 11, 14, 16, 14, 15, 8, 17, 18, 19]\n",
      "[0, 10, 2, 3, 4, 5, 4, 7, 3, 9, 10, 11, 12, 3, 14, 15, 5, 17, 5, 19]\n",
      "[12, 1, 2, 3, 4, 6, 6, 7, 8, 9, 10, 3, 12, 13, 9, 15, 16, 17, 18, 19]\n",
      "[12, 1, 2, 3, 9, 8, 6, 7, 8, 5, 1, 18, 16, 13, 1, 19, 16, 17, 18, 19]\n",
      "[0, 1, 6, 3, 4, 5, 19, 8, 8, 9, 10, 11, 12, 14, 14, 1, 16, 11, 1, 19]\n",
      "[0, 17, 4, 3, 15, 5, 6, 7, 3, 9, 10, 11, 12, 13, 14, 3, 14, 17, 1, 3]\n",
      "[1, 1, 2, 17, 7, 19, 18, 7, 8, 9, 3, 16, 5, 19, 14, 15, 16, 17, 6, 3]\n",
      "[0, 12, 7, 3, 4, 17, 6, 7, 8, 16, 10, 0, 12, 13, 14, 15, 3, 17, 18, 19]\n",
      "[0, 12, 2, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 2, 16, 17, 18, 19]\n",
      "[4, 1, 2, 3, 4, 5, 19, 18, 8, 9, 1, 11, 12, 13, 4, 15, 16, 2, 18, 19]\n",
      "[7, 9, 2, 18, 8, 5, 6, 7, 19, 9, 19, 11, 12, 13, 14, 15, 16, 11, 18, 19]\n",
      "[11, 2, 0, 3, 4, 5, 6, 7, 19, 9, 2, 7, 12, 4, 14, 15, 6, 10, 18, 19]\n",
      "[0, 1, 2, 3, 4, 15, 9, 5, 12, 6, 10, 11, 12, 18, 19, 15, 16, 17, 9, 19]\n",
      "[6, 7, 2, 19, 4, 5, 6, 7, 4, 2, 17, 7, 12, 3, 3, 3, 16, 7, 18, 19]\n",
      "[0, 1, 2, 12, 4, 5, 6, 17, 7, 9, 10, 11, 16, 13, 14, 15, 12, 5, 18, 19]\n",
      "[0, 0, 2, 3, 4, 12, 16, 7, 5, 18, 12, 15, 12, 1, 14, 15, 16, 17, 18, 2]\n",
      "[0, 1, 15, 3, 8, 5, 6, 7, 8, 9, 10, 10, 5, 13, 18, 15, 7, 17, 18, 19]\n",
      "[18, 1, 2, 3, 14, 5, 6, 13, 8, 13, 10, 8, 8, 13, 1, 1, 16, 17, 11, 19]\n",
      "[0, 1, 2, 3, 1, 5, 6, 16, 2, 9, 10, 8, 15, 13, 14, 1, 16, 17, 12, 19]\n",
      "[12, 17, 17, 10, 4, 5, 6, 7, 8, 10, 17, 11, 19, 13, 14, 1, 16, 17, 6, 19]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-8c27012bf10f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-0e93c701e032>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, loss, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#         writer.add_scalar(\"F1/train\", f1_score(metric_y,metric_p), epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric_y_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_p_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric_y_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_p_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;31m#         writer.add_scalar(\"Loss/valid\", loss_val, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#         writer.add_scalar(\"Acc/valid\", val_accuracy, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-0e93c701e032>\u001b[0m in \u001b[0;36mcompute_valid\u001b[0;34m(model, loader, loss, epoch, metric_y, metric_p)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mloss_accum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mimgs_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hdd1/brain/BraTS19/YandexCup/src/train/CustomDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get_image_batch take shape count_i,img_dat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mimg_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mimg_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hdd1/brain/BraTS19/YandexCup/ru-clip/clip/model.py\u001b[0m in \u001b[0;36mget_image_batch\u001b[0;34m(img_paths, img_transform, args)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \"\"\"\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1941\u001b[0m                 )\n\u001b[1;32m   1942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1943\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = train_model(model, train_dl, val_dl, loss, optimizer, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "frank-strip",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-98d05e62ba4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "np.arange(5).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-replacement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
