{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handled-difference",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor.kumpan/.miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import Sent2textDataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/data/hdd1/brain/BraTS19/YandexCup/ru-clip\")\n",
    "\n",
    "import pandas as pd\n",
    "from clip.evaluate.utils import (\n",
    "    load_weights_only,\n",
    "    show_similarity,\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "from CustomDataset import Sent2textDataset\n",
    "model, args = load_weights_only(\"ViT-B/32-small\",seq_length = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secondary-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "def sempler(data_train, batch_size = 4,split = .8):\n",
    "    \n",
    "    data_size = len(data_train)\n",
    "\n",
    "    validation_split = split\n",
    "    split = int(np.floor(validation_split * data_size))\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices,val_indices = indices[split:],indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                              sampler=train_sampler,)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                            sampler=val_sampler,)\n",
    "\n",
    "    return train_loader,val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conservative-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    val_loss_hist = []\n",
    "    metric_y_val = metric_p_val = None\n",
    "    \n",
    "#     scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,\n",
    "#                                                   T_0=40, \n",
    "#                                                   T_mult=2,\n",
    "#                                                   eta_min=1e-9)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        \n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        loss_accum = 0\n",
    "        for i_step, data  in enumerate(train_loader):\n",
    "            \n",
    "            \n",
    "                imgs_gpu = torch.squeeze(data[0].cuda(),1)\n",
    "                texts_gpu = data[1].cuda()\n",
    "                att_mask_gpu = data[2].cuda()\n",
    "                label_gpu = torch.arange(data[0].shape[0]).cuda()\n",
    "                \n",
    "                \n",
    "                _, prediction = model(img_input={\"x\": imgs_gpu},\n",
    "                                    text_input={\"x\": texts_gpu, \"attention_mask\":att_mask_gpu})\n",
    "            \n",
    "                loss_value = loss(prediction, label_gpu)\n",
    "                \n",
    "                _, preds = torch.max(prediction, 1)\n",
    "                \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "                if i_step == 0 and epoch == 0:\n",
    "                    metric_y = label_gpu.cpu().numpy()\n",
    "                    metric_p = preds.cpu().numpy()\n",
    "                else:\n",
    "                    metric_y = np.concatenate((metric_y, label_gpu.cpu().numpy()))\n",
    "                    metric_p = np.concatenate((metric_p, preds.cpu().numpy())) \n",
    "                    \n",
    "                correct_samples += torch.sum(preds == label_gpu)\n",
    "                loss_accum += loss_value\n",
    "                total_samples += label_gpu.shape[0]\n",
    "                \n",
    "                del imgs_gpu\n",
    "                del texts_gpu\n",
    "                del att_mask_gpu\n",
    "            \n",
    "                del label_gpu\n",
    "        \n",
    "        \n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = correct_samples / total_samples\n",
    "#         writer.add_scalar(\"Loss/train\", ave_loss, epoch)\n",
    "#         writer.add_scalar(\"Acc/train\", train_accuracy, epoch)\n",
    "#         writer.add_scalar(\"F1/train\", f1_score(metric_y,metric_p), epoch)\n",
    "\n",
    "        val_accuracy, loss_val,metric_y_val, metric_p_val = compute_valid(model, val_loader, loss, epoch,metric_y_val, metric_p_val)\n",
    "#         writer.add_scalar(\"Loss/valid\", loss_val, epoch)\n",
    "#         writer.add_scalar(\"Acc/valid\", val_accuracy, epoch)\n",
    "#         writer.add_scalar(\"F1/valid\", f1_score(metric_y_val,metric_p_val), epoch)\n",
    "        \n",
    "#         writer.add_scalar(\"Lr/epoch\", scheduler.get_last_lr()[-1], epoch)\n",
    "#         scheduler.step(epoch)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        val_loss_hist.append(loss_val)\n",
    "        \n",
    "\n",
    "        print(\"Average loss: %f, Val loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss,loss_val, train_accuracy, val_accuracy))\n",
    "    return metric_y_val, metric_p_val\n",
    "#         print('Epoch:', epoch, 'LR:', scheduler.get_last_lr())\n",
    "\n",
    "def compute_valid(model, loader, loss,epoch, metric_y= None, metric_p=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        loss_accum = 0\n",
    "        \n",
    "        for i_step, data in enumerate(loader):\n",
    "            \n",
    "            imgs_gpu = torch.squeeze(data[0].cuda(),1)\n",
    "            texts_gpu = data[1].cuda()\n",
    "            att_mask_gpu = data[2].cuda()\n",
    "            label_gpu = torch.arange(data[0].shape[0]).cuda()\n",
    "\n",
    "            _, prediction = model(img_input={\"x\": imgs_gpu},\n",
    "                                    text_input={\"x\": texts_gpu, \"attention_mask\":att_mask_gpu})\n",
    "            \n",
    "            \n",
    "            loss_value = loss(prediction, label_gpu)\n",
    "            _, preds = torch.max(prediction, 1)\n",
    "            \n",
    "            \n",
    "            if i_step == 0 and epoch == 0:\n",
    "                metric_y = label_gpu.cpu().numpy()\n",
    "                metric_p = preds.cpu().numpy()\n",
    "            else:\n",
    "                metric_y = np.concatenate((metric_y, label_gpu.cpu().numpy()))\n",
    "                metric_p = np.concatenate((metric_p, preds.cpu().numpy())) \n",
    "            \n",
    "            \n",
    "            correct_samples += torch.sum(preds == label_gpu)\n",
    "            total_samples += label_gpu.shape[0]\n",
    "            loss_accum += loss_value\n",
    "\n",
    "            del imgs_gpu\n",
    "            del texts_gpu\n",
    "            del att_mask_gpu\n",
    "            \n",
    "            del label_gpu\n",
    "                \n",
    "        loss_val = loss_accum / (i_step + 1)\n",
    "        val_accuracy = correct_samples / total_samples\n",
    "        return val_accuracy, loss_val, metric_y, metric_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "heavy-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5462418it [00:11, 477370.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broken 0 images\n",
      "2251 images in folder from 2500 in csv file\n"
     ]
    }
   ],
   "source": [
    "path_t_csv = pd.read_csv(\"data/60k_data_preproc.csv\")[:2500] #\n",
    "path_i_json = \"data/images.json\"\n",
    "path_i_folder = \"data/images\"\n",
    "\n",
    "\n",
    "# в csv file для каждого касса должен быть массив\n",
    "\n",
    "\n",
    "ds = Sent2textDataset(path_t_csv,\n",
    "                      path_i_json,path_i_folder,\n",
    "                      down_data = False,\n",
    "                      n_classes = 5,args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hidden-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "through-cincinnati",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual_encoder): VisualEncoder(\n",
       "    (model): VisualTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (projection): Projection(\n",
       "      (linear1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (text_encoder): TextEncoder(\n",
       "    (model): GPT2Model(\n",
       "      (wte): Embedding(50264, 768)\n",
       "      (wpe): Embedding(2048, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (projection): Projection(\n",
       "      (linear1): Linear(in_features=768, out_features=1024, bias=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "documented-arabic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logit_scale',\n",
       " 'visual_encoder.projection.linear1.weight',\n",
       " 'visual_encoder.projection.linear2.weight',\n",
       " 'visual_encoder.projection.layer_norm.weight',\n",
       " 'visual_encoder.projection.layer_norm.bias',\n",
       " 'text_encoder.model.h.11.ln_1.weight',\n",
       " 'text_encoder.model.h.11.ln_1.bias',\n",
       " 'text_encoder.model.h.11.attn.c_attn.weight',\n",
       " 'text_encoder.model.h.11.attn.c_attn.bias',\n",
       " 'text_encoder.model.h.11.attn.c_proj.weight',\n",
       " 'text_encoder.model.h.11.attn.c_proj.bias',\n",
       " 'text_encoder.model.h.11.ln_2.weight',\n",
       " 'text_encoder.model.h.11.ln_2.bias',\n",
       " 'text_encoder.model.h.11.mlp.c_fc.weight',\n",
       " 'text_encoder.model.h.11.mlp.c_fc.bias',\n",
       " 'text_encoder.model.h.11.mlp.c_proj.weight',\n",
       " 'text_encoder.model.h.11.mlp.c_proj.bias',\n",
       " 'text_encoder.projection.linear1.weight',\n",
       " 'text_encoder.projection.linear2.weight',\n",
       " 'text_encoder.projection.layer_norm.weight',\n",
       " 'text_encoder.projection.layer_norm.bias']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.cuda().float()\n",
    "for name,param in model.named_parameters():\n",
    "    if name.find(\"visual_encoder\") != -1:\n",
    "        param.requires_grad = False\n",
    "    if name.find(\"text_encoder\")!= -1:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    if name.find(\"11\")!=-1 and name.find(\"visual_encoder\")== -1:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    if name.find(\"projection\")!=-1:\n",
    "        param.requires_grad = True\n",
    "    if name == \"logit_scale\":\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    \n",
    "[name for name,param in model.named_parameters() if param.requires_grad ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "logical-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLoss()\n",
    "optimizer = Adam([param for param in model.parameters() if param.requires_grad], lr = 1e-04, weight_decay = 1e-05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minute-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,val_dl =  sempler(ds, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Average loss: 1.124895, Val loss: 1.166016, Train accuracy: 0.662971, Val accuracy: 0.686111\n",
      "1\n",
      "Average loss: 0.989753, Val loss: 1.218044, Train accuracy: 0.696231, Val accuracy: 0.666111\n",
      "2\n",
      "Average loss: 1.011923, Val loss: 1.202927, Train accuracy: 0.687361, Val accuracy: 0.676111\n",
      "3\n",
      "Average loss: 0.983407, Val loss: 1.186103, Train accuracy: 0.711752, Val accuracy: 0.676667\n",
      "4\n",
      "Average loss: 0.878538, Val loss: 1.146035, Train accuracy: 0.707317, Val accuracy: 0.675556\n",
      "5\n",
      "Average loss: 0.921292, Val loss: 1.170126, Train accuracy: 0.700665, Val accuracy: 0.682222\n",
      "6\n",
      "Average loss: 0.883479, Val loss: 1.155602, Train accuracy: 0.707317, Val accuracy: 0.673333\n",
      "7\n",
      "Average loss: 0.839144, Val loss: 1.149263, Train accuracy: 0.725055, Val accuracy: 0.676111\n",
      "8\n",
      "Average loss: 0.791811, Val loss: 1.150697, Train accuracy: 0.753880, Val accuracy: 0.690556\n",
      "9\n",
      "Average loss: 0.779538, Val loss: 1.140624, Train accuracy: 0.731707, Val accuracy: 0.683333\n",
      "10\n",
      "Average loss: 0.695124, Val loss: 1.174814, Train accuracy: 0.767184, Val accuracy: 0.686111\n",
      "11\n",
      "Average loss: 0.707942, Val loss: 1.131752, Train accuracy: 0.760532, Val accuracy: 0.694444\n",
      "12\n",
      "Average loss: 0.742672, Val loss: 1.145695, Train accuracy: 0.758315, Val accuracy: 0.672778\n",
      "13\n",
      "Average loss: 0.742103, Val loss: 1.129588, Train accuracy: 0.736142, Val accuracy: 0.682778\n",
      "14\n",
      "Average loss: 0.609538, Val loss: 1.097890, Train accuracy: 0.802661, Val accuracy: 0.685000\n",
      "15\n",
      "Average loss: 0.650068, Val loss: 1.132927, Train accuracy: 0.773836, Val accuracy: 0.686667\n",
      "16\n",
      "Average loss: 0.675159, Val loss: 1.124654, Train accuracy: 0.767184, Val accuracy: 0.687222\n",
      "17\n",
      "Average loss: 0.574746, Val loss: 1.128137, Train accuracy: 0.818182, Val accuracy: 0.686667\n",
      "18\n",
      "Average loss: 0.587857, Val loss: 1.111378, Train accuracy: 0.800443, Val accuracy: 0.698889\n",
      "19\n",
      "Average loss: 0.650092, Val loss: 1.188125, Train accuracy: 0.773836, Val accuracy: 0.684444\n",
      "20\n",
      "Average loss: 0.521876, Val loss: 1.178478, Train accuracy: 0.815965, Val accuracy: 0.672222\n",
      "21\n",
      "Average loss: 0.585269, Val loss: 1.148676, Train accuracy: 0.789357, Val accuracy: 0.682222\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "result = train_model(model, train_dl, val_dl, loss, optimizer, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-contribution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
